{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Performance comparisons\n",
    "\n",
    "In memory and out of memory, using dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Change dir to repo root if running from repo (rather than pip installed)\n",
    "# (Assuming running from [repo]/scripts/)\n",
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "from incremental_trees.trees import StreamingRFC\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import dask_ml.datasets\n",
    "from dask_ml.wrappers import Incremental\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask_ml.model_selection import train_test_split as dask_tts\n",
    "\n",
    "import dask as dd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "MAX_ESTIMATORS = 60  # Lower to run faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-01 21:04:41,365 - distributed.deploy.spec - WARNING - Cluster closed without starting up\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cluster failed to start: Scheduler failed to start.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\envs\\IncrementalTrees\\lib\\site-packages\\distributed\\core.py:524\u001B[0m, in \u001B[0;36mServer.start\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 524\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mwait_for(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstart_unsafe(), timeout\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[0;32m    525\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mTimeoutError \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IncrementalTrees\\lib\\asyncio\\tasks.py:442\u001B[0m, in \u001B[0;36mwait_for\u001B[1;34m(fut, timeout, loop)\u001B[0m\n\u001B[0;32m    441\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 442\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m fut\n\u001B[0;32m    444\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IncrementalTrees\\lib\\site-packages\\distributed\\scheduler.py:3880\u001B[0m, in \u001B[0;36mScheduler.start_unsafe\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   3879\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m addr \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_address:\n\u001B[1;32m-> 3880\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlisten(\n\u001B[0;32m   3881\u001B[0m         addr,\n\u001B[0;32m   3882\u001B[0m         allow_offload\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m   3883\u001B[0m         handshake_overrides\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpickle-protocol\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m4\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompression\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mNone\u001B[39;00m},\n\u001B[0;32m   3884\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msecurity\u001B[38;5;241m.\u001B[39mget_listen_args(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscheduler\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   3885\u001B[0m     )\n\u001B[0;32m   3886\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mip \u001B[38;5;241m=\u001B[39m get_address_host(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlisten_address)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IncrementalTrees\\lib\\site-packages\\distributed\\core.py:707\u001B[0m, in \u001B[0;36mServer.listen\u001B[1;34m(self, port_or_addr, allow_offload, **kwargs)\u001B[0m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(addr, \u001B[38;5;28mstr\u001B[39m)\n\u001B[1;32m--> 707\u001B[0m listener \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m listen(\n\u001B[0;32m    708\u001B[0m     addr,\n\u001B[0;32m    709\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_comm,\n\u001B[0;32m    710\u001B[0m     deserialize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeserialize,\n\u001B[0;32m    711\u001B[0m     allow_offload\u001B[38;5;241m=\u001B[39mallow_offload,\n\u001B[0;32m    712\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    713\u001B[0m )\n\u001B[0;32m    714\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlisteners\u001B[38;5;241m.\u001B[39mappend(listener)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IncrementalTrees\\lib\\site-packages\\distributed\\comm\\core.py:212\u001B[0m, in \u001B[0;36mListener.__await__.<locals>._\u001B[1;34m()\u001B[0m\n\u001B[0;32m    211\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_\u001B[39m():\n\u001B[1;32m--> 212\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstart()\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IncrementalTrees\\lib\\site-packages\\distributed\\comm\\tcp.py:580\u001B[0m, in \u001B[0;36mBaseTCPListener.start\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    575\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    576\u001B[0m     \u001B[38;5;66;03m# When shuffling data between workers, there can\u001B[39;00m\n\u001B[0;32m    577\u001B[0m     \u001B[38;5;66;03m# really be O(cluster size) connection requests\u001B[39;00m\n\u001B[0;32m    578\u001B[0m     \u001B[38;5;66;03m# on a single worker socket, make sure the backlog\u001B[39;00m\n\u001B[0;32m    579\u001B[0m     \u001B[38;5;66;03m# is large enough not to lose any.\u001B[39;00m\n\u001B[1;32m--> 580\u001B[0m     sockets \u001B[38;5;241m=\u001B[39m \u001B[43mnetutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbind_sockets\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    581\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maddress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mip\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbacklog\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbacklog\u001B[49m\n\u001B[0;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    583\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    584\u001B[0m     \u001B[38;5;66;03m# EADDRINUSE can happen sporadically when trying to bind\u001B[39;00m\n\u001B[0;32m    585\u001B[0m     \u001B[38;5;66;03m# to an ephemeral port\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IncrementalTrees\\lib\\site-packages\\tornado\\netutil.py:162\u001B[0m, in \u001B[0;36mbind_sockets\u001B[1;34m(port, address, family, backlog, flags, reuse_port)\u001B[0m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 162\u001B[0m     \u001B[43msock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbind\u001B[49m\u001B[43m(\u001B[49m\u001B[43msockaddr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[1;31mOSError\u001B[0m: [WinError 10048] Only one usage of each socket address (protocol/network address/port) is normally permitted",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\envs\\IncrementalTrees\\lib\\site-packages\\distributed\\deploy\\spec.py:309\u001B[0m, in \u001B[0;36mSpecCluster._start\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    308\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscheduler \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscheduler_spec\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moptions\u001B[39m\u001B[38;5;124m\"\u001B[39m, {}))\n\u001B[1;32m--> 309\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscheduler \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscheduler\n\u001B[0;32m    310\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscheduler_comm \u001B[38;5;241m=\u001B[39m rpc(\n\u001B[0;32m    311\u001B[0m     \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscheduler, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexternal_address\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    312\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscheduler\u001B[38;5;241m.\u001B[39maddress,\n\u001B[0;32m    313\u001B[0m     connection_args\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msecurity\u001B[38;5;241m.\u001B[39mget_connection_args(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclient\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    314\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IncrementalTrees\\lib\\site-packages\\distributed\\core.py:532\u001B[0m, in \u001B[0;36mServer.start\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    531\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m _close_on_failure(exc)\n\u001B[1;32m--> 532\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m failed to start.\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n\u001B[0;32m    533\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m Status\u001B[38;5;241m.\u001B[39mrunning\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Scheduler failed to start.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Prepare dask cluster\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m cluster \u001B[38;5;241m=\u001B[39m \u001B[43mLocalCluster\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprocesses\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_workers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mthreads_per_worker\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscheduler_port\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8383\u001B[39;49m\n\u001B[0;32m      7\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m client \u001B[38;5;241m=\u001B[39m Client(cluster)\n\u001B[0;32m      9\u001B[0m client\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IncrementalTrees\\lib\\site-packages\\distributed\\deploy\\local.py:253\u001B[0m, in \u001B[0;36mLocalCluster.__init__\u001B[1;34m(self, name, n_workers, threads_per_worker, processes, loop, start, host, ip, scheduler_port, silence_logs, dashboard_address, worker_dashboard_address, diagnostics_port, services, worker_services, service_kwargs, asynchronous, security, protocol, blocked_handlers, interface, worker_class, scheduler_kwargs, scheduler_sync_interval, **worker_kwargs)\u001B[0m\n\u001B[0;32m    250\u001B[0m worker \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcls\u001B[39m\u001B[38;5;124m\"\u001B[39m: worker_class, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moptions\u001B[39m\u001B[38;5;124m\"\u001B[39m: worker_kwargs}\n\u001B[0;32m    251\u001B[0m workers \u001B[38;5;241m=\u001B[39m {i: worker \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_workers)}\n\u001B[1;32m--> 253\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworker\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworker\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    259\u001B[0m \u001B[43m    \u001B[49m\u001B[43masynchronous\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43masynchronous\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    260\u001B[0m \u001B[43m    \u001B[49m\u001B[43msilence_logs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilence_logs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    261\u001B[0m \u001B[43m    \u001B[49m\u001B[43msecurity\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msecurity\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    262\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscheduler_sync_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscheduler_sync_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    263\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IncrementalTrees\\lib\\site-packages\\distributed\\deploy\\spec.py:275\u001B[0m, in \u001B[0;36mSpecCluster.__init__\u001B[1;34m(self, workers, scheduler, worker, asynchronous, loop, security, silence_logs, name, shutdown_on_close, scheduler_sync_interval)\u001B[0m\n\u001B[0;32m    273\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m called_from_running_loop:\n\u001B[0;32m    274\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_loop_runner\u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m--> 275\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msync\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_start\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    276\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    277\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_correct_state)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IncrementalTrees\\lib\\site-packages\\distributed\\utils.py:339\u001B[0m, in \u001B[0;36mSyncMethodMixin.sync\u001B[1;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001B[0m\n\u001B[0;32m    337\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m future\n\u001B[0;32m    338\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 339\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m sync(\n\u001B[0;32m    340\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloop, func, \u001B[38;5;241m*\u001B[39margs, callback_timeout\u001B[38;5;241m=\u001B[39mcallback_timeout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    341\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IncrementalTrees\\lib\\site-packages\\distributed\\utils.py:406\u001B[0m, in \u001B[0;36msync\u001B[1;34m(loop, func, callback_timeout, *args, **kwargs)\u001B[0m\n\u001B[0;32m    404\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m error:\n\u001B[0;32m    405\u001B[0m     typ, exc, tb \u001B[38;5;241m=\u001B[39m error\n\u001B[1;32m--> 406\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc\u001B[38;5;241m.\u001B[39mwith_traceback(tb)\n\u001B[0;32m    407\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    408\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IncrementalTrees\\lib\\site-packages\\distributed\\utils.py:379\u001B[0m, in \u001B[0;36msync.<locals>.f\u001B[1;34m()\u001B[0m\n\u001B[0;32m    377\u001B[0m         future \u001B[38;5;241m=\u001B[39m asyncio\u001B[38;5;241m.\u001B[39mwait_for(future, callback_timeout)\n\u001B[0;32m    378\u001B[0m     future \u001B[38;5;241m=\u001B[39m asyncio\u001B[38;5;241m.\u001B[39mensure_future(future)\n\u001B[1;32m--> 379\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01myield\u001B[39;00m future\n\u001B[0;32m    380\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    381\u001B[0m     error \u001B[38;5;241m=\u001B[39m sys\u001B[38;5;241m.\u001B[39mexc_info()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IncrementalTrees\\lib\\site-packages\\tornado\\gen.py:769\u001B[0m, in \u001B[0;36mRunner.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    766\u001B[0m exc_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    768\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 769\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    770\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    771\u001B[0m     exc_info \u001B[38;5;241m=\u001B[39m sys\u001B[38;5;241m.\u001B[39mexc_info()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\IncrementalTrees\\lib\\site-packages\\distributed\\deploy\\spec.py:319\u001B[0m, in \u001B[0;36mSpecCluster._start\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m Status\u001B[38;5;241m.\u001B[39mfailed\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close()\n\u001B[1;32m--> 319\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCluster failed to start: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Cluster failed to start: Scheduler failed to start."
     ]
    }
   ],
   "source": [
    "# Prepare dask cluster\n",
    "cluster = LocalCluster(\n",
    "    processes=False,\n",
    "    n_workers=2,\n",
    "    threads_per_worker=2,\n",
    "    scheduler_port=8383\n",
    ")\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Synthetic data, in memory\n",
    "\n",
    "Compare increasing estimators with RandomForest (using warm_start) against Incremental StreamingRFC (dask handles .partial_fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x, y = dask_ml.datasets.make_blobs(\n",
    "    n_samples=1e5,\n",
    "    chunks=1e4,\n",
    "    random_state=0,\n",
    "    n_features=40,\n",
    "    centers=2,\n",
    "    cluster_std=100\n",
    ")\n",
    "\n",
    "x_dd = dd.dataframe.from_array(x, chunksize=1e4)\n",
    "y_dd = dd.dataframe.from_array(y, chunksize=1e4)\n",
    "\n",
    "x_pd = pd.DataFrame(x.persist().compute())\n",
    "y_pd = pd.DataFrame(y.persist().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_pd.memory_usage(deep=True).sum() / 1024 / 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Standard random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def score(mod, train: Tuple[np.array, np.array], test: Tuple[np.array, np.array], pr=False) -> Tuple[float, float]:\n",
    "    \"\"\"Return ROC auc on x_train and x_test (from caller) on mod. Print if requested.\"\"\"\n",
    "\n",
    "    y_pred_train_proba = mod.predict_proba(train[0])[:, 1]\n",
    "    y_pred_test_proba = mod.predict_proba(test[0])[:, 1]\n",
    "\n",
    "    roc_train = roc_auc_score(train[1], y_pred_train_proba)\n",
    "    roc_test = roc_auc_score(test[1], y_pred_test_proba)\n",
    "    if pr:\n",
    "        print(f\"n_ests: {len(rfc.estimators_)}\")\n",
    "        print(f'Train AUC: {roc_train}')\n",
    "        print(f'Test AUC: {roc_test}')\n",
    "\n",
    "    return roc_train, roc_test\n",
    "\n",
    "\n",
    "def score_dask(mod, train: Tuple[np.array, np.array], test: Tuple[np.array, np.array], pr=False) -> Tuple[float, float]:\n",
    "    \"\"\"Score model using available dask metric (accuracy).\"\"\"\n",
    "\n",
    "    roc_train = mod.score(train[0], train[1])\n",
    "    roc_test = mod.score(test[0], test[1])\n",
    "    if pr:\n",
    "        print(f\"n_ests: {len(rfc.estimators_)}\")\n",
    "        print(f'Train AUC: {roc_train}')\n",
    "        print(f'Test AUC: {roc_test}')\n",
    "\n",
    "    return roc_train, roc_test\n",
    "\n",
    "\n",
    "def multiple_fit(x: np.array, y: np.array, steps=np.arange(1, 101, 2), sample: int = 1):\n",
    "    \"\"\"\n",
    "    Fit a random forest model with an increasing number of estimators.\n",
    "    \n",
    "    This version doesn't use warm start and refits the model from scratch each iteration.\n",
    "    This is for the sake of comparing timings to dask function below.\n",
    "    \n",
    "    :param steps: Range to iterate over. Sets total number of estimators that will be fit in model\n",
    "                  after each iteration. Should be range with constant step size.\n",
    "    :param sample: Proportion of randomly sampled training data to use on each partial_fit call.\n",
    "                   If sample = 1, all training data is used on each interation,\n",
    "                   so should behave as standard random forest. Default = 1 (100%).\n",
    "    \"\"\"\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1)\n",
    "\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    for s in steps:\n",
    "        # Fit full model on each iteration\n",
    "        rfc = RandomForestClassifier(warm_start=False)\n",
    "\n",
    "        # Fit model with these n ests\n",
    "        rfc.set_params(n_estimators=s)\n",
    "        rfc.fit(x_train, y_train)\n",
    "\n",
    "        tr_score, te_score = score(rfc, train=(x_train, y_train), test=(x_test, y_test), pr=False)\n",
    "\n",
    "        train_scores.append(tr_score)\n",
    "        test_scores.append(te_score)\n",
    "\n",
    "    return rfc, train_scores, test_scores\n",
    "\n",
    "\n",
    "def plot_auc(steps, train_scores, test_scores):\n",
    "    \"\"\"Plot the train and test auc scores vs total number of model estimators\"\"\"\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.plot(steps, train_scores)\n",
    "    plt.plot(steps, test_scores)\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('auc')\n",
    "    plt.legend(['train', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "steps = np.arange(1, MAX_ESTIMATORS, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(warm_start=True)\n",
    "\n",
    "%time rfc, train_scores, test_scores = multiple_fit(x_pd.values, y_pd.values.squeeze(), steps=steps)\n",
    "\n",
    "print(f\"With {len(rfc.estimators_)}: {train_scores[-1]} | {test_scores[-1]}\")\n",
    "plot_auc(steps, train_scores, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Single incremental forest specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = dask_tts(x, y, test_size=0.25)\n",
    "\n",
    "x_train.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Incremental forest\n",
    "1 estimator per subset, 10 % per chunk, 1 pass through data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "srfc = Incremental(StreamingRFC(n_estimators_per_chunk=1, max_n_estimators=np.inf))\n",
    "\n",
    "srfc.fit(x_train, y_train, classes=[0, 1])\n",
    "\n",
    "tr_score, te_score = score(srfc, train=(x_train, y_train), test=(x_test, y_test), pr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Incremental forest\n",
    "20 estimators per subset (different features), 10 % per chunk, 1 pass through data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "srfc = Incremental(StreamingRFC(n_estimators_per_chunk=20, max_n_estimators=np.inf))\n",
    "\n",
    "srfc.fit(x_train, y_train, classes=[0, 1])\n",
    "\n",
    "tr_score, te_score = score(srfc, train=(x_train, y_train), test=(x_test, y_test), pr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Forest of partial decision trees\n",
    "1 estimator per subset with all features, 10 % per chunk, 1 pass through data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "srfc = Incremental(StreamingRFC(\n",
    "    n_estimators_per_chunk=1,\n",
    "    max_n_estimators=np.max(steps),\n",
    "    max_features=x.shape[1])\n",
    ")\n",
    "\n",
    "srfc.fit(x_train, y_train,\n",
    "         classes=[0, 1])\n",
    "\n",
    "tr_score, te_score = score(srfc, train=(x_train, y_train), test=(x_test, y_test), pr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Forest of partial decision trees\n",
    "20 estimator per subset with all features, 10 % per chunk, 1 pass through data.\n",
    "\n",
    "Extra estimators shouldn't help here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "srfc = Incremental(StreamingRFC(\n",
    "    n_estimators_per_chunk=20,\n",
    "    max_n_estimators=np.max(steps),\n",
    "    max_features=x.shape[1])\n",
    ")\n",
    "\n",
    "srfc.fit(x_train, y_train, classes=[0, 1])\n",
    "\n",
    "tr_score, te_score = score(srfc, train=(x_train, y_train), test=(x_test, y_test), pr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### n estimators per chunk vs performance\n",
    "\n",
    "Effect of increasing estimators per subset (with different set ups)\n",
    "\n",
    "Function here add Incremental to supplied model, and uses .fit to refit the full model in each iteration.\n",
    "\n",
    "The other functions (above and in PerformanceComparisons.ipynb) do incremental fits using warm start (either directly or via .partial_fit). \n",
    "\n",
    "This means the timing information cannot be directly compared!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def multiple_dask_fit(x: np.ndarray, y: np.ndarray, steps=np.arange(1, 101, 2),\n",
    "                      **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Fit increasing number of estimators using .partial_fit on a subsample of the training data.\n",
    "    Uses Dask by adding Incremental to model and calling fit. This refits the whole model one each\n",
    "    iteration, so will be slower than the other test functions. Timing this function can only be compared\n",
    "    to other calls of this function.\n",
    "    \n",
    "    The data passed to the Random forest fit by partial_fit is handled by dask and is sequential batches\n",
    "    of data, rather than random samples (as used by inc_partial_fit in PerformanceComparisons.ipynb).\n",
    "    \n",
    "    StreamingRFC.n_estimators: Number of estimators that will be fit in each step. Set from first\n",
    "                               difference in range (ie. range[1]-range[0])\n",
    "    StreamingRFC.max_n_estimators: Limit on number of estimators than will be fit in model. Should >\n",
    "                                   range[-1].\n",
    "    \n",
    "    :param steps: Range to iterate over. Sets total number of estimators that will be fit in model\n",
    "                  after each iteration. Should be range with constant step size.\n",
    "    \"\"\"\n",
    "\n",
    "    x_train, x_test, y_train, y_test = dask_tts(x, y, test_size=0.25)\n",
    "\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    for s in steps:\n",
    "        # Create fresh model each iteration\n",
    "        srfc_ = StreamingRFC(n_estimators_per_chunk=s, max_n_estimators=np.inf, **kwargs)\n",
    "\n",
    "        # Add Incremental\n",
    "        srfc_ = Incremental(srfc_)\n",
    "\n",
    "        # Fit model with these n ests\n",
    "        # From scratch each time\n",
    "        srfc_.fit(x_train, y_train,\n",
    "                  classes=[0, 1])\n",
    "\n",
    "        tr_score, te_score = score(\n",
    "            srfc_,\n",
    "            train=(x_train, y_train),\n",
    "            test=(x_test, y_test),\n",
    "            pr=False)\n",
    "\n",
    "        train_scores.append(tr_score)\n",
    "        test_scores.append(te_score)\n",
    "\n",
    "    return srfc_, train_scores, test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Incremental forest\n",
    "*range* estimators per subset (different features), 10 % per chunk, 1 pass through data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "steps = np.arange(1, MAX_ESTIMATORS, 6)\n",
    "\n",
    "%time final_est, train_scores, test_scores = multiple_dask_fit(x, y, steps=steps)\n",
    "print(f\"With {len(final_est.estimators_)}: {train_scores[-1]} | {test_scores[-1]}\")\n",
    "plot_auc(steps, train_scores, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Forest of partial decision trees\n",
    "*range* estimators per subset with all features, 10 % per chunk, 1 pass through data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "steps = np.arange(1, MAX_ESTIMATORS, 6)\n",
    "\n",
    "%time final_est, train_scores, test_scores = multiple_dask_fit(x, y, steps=steps, max_features=x.shape[1])\n",
    "print(f\"With {len(final_est.estimators_)}: {train_scores[-1]} | {test_scores[-1]}\")\n",
    "plot_auc(steps, train_scores, test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}